# GenerativeAI_Text_To_Video

This project is a text-to-video generation pipeline that transforms textual prompts into high-quality videos. It leverages Tencent's HunyuanVideo model for video generation and Real-ESRGAN for video upscaling.

# Overview
The pipeline converts textual descriptions into coherent videos, enhancing them for clarity and resolution.

# Model Architecture
## HunyuanVideo
HunyuanVideo is a 13B parameter diffusion transformer model designed to be competitive with closed-source video foundation models and enable wider community access. This model uses a ‚Äúdual-stream to single-stream‚Äù architecture to separately process the video and text tokens first, before concatenating and feeding them to the transformer to fuse the multimodal information. A pretrained multimodal large language model (MLLM) is used as the encoder because it has better image-text alignment, better image detail description and reasoning, and it can be used as a zero-shot learner if system instructions are added to user prompts. Finally, HunyuanVideo uses a 3D causal variational autoencoder to more efficiently process video data at the original resolution and frame rate.

<img width="1415" alt="Image" src="https://github.com/user-attachments/assets/737762ed-d241-4f98-95a8-e9d0bf6d04e7" />

1. Text Prompt Input
A user provides a natural language prompt that describes the desired video scene.
_Example: "Sunny day, freedom and adventure, high motion. A man with a beard riding a motorcycle on the street."_

2. Text Embedding with Large Language Model (LLM)
The text prompt is encoded into a sequence of text tokens using a large multimodal language model (LLM). These tokens capture semantic meaning and temporal cues.

3. Video Compression (Training Phase Only)
During training, ground-truth videos are encoded into a compact latent space using a Hunyuan Causal 3D Variational Autoencoder (3DVAE) encoder. This compresses the high-dimensional video data into manageable latent representations.

4. Noise Addition (Diffusion Process)
Gaussian noise is added to the latent video representations. This is a standard process in diffusion models, where the goal is to learn how to reverse the noise and recover the clean latent.

5. Dual-Stream Transformer Input
Both noisy latent video tokens and encoded text tokens are fed into the HunyuanVideo Diffusion Backbone, a transformer-based model. The dual-stream attention mechanism allows the model to effectively fuse spatial-temporal video features with text semantics.

6. Latent Video Output (Denoising)
The diffusion model denoises the input tokens step-by-step, generating a refined latent video representation conditioned on the text prompt.

7. Final Video Generation
The denoised latent video is decoded back into high-resolution video frames using the Hunyuan Causal 3DVAE Decoder, completing the text-to-video generation pipeline.

## ESRGAN : Enhanced Super-Resolution Generative Adversarial Networks

1. Input: Low-Resolution Image
The model takes a low-resolution (LR) image as input, often downsampled using bicubic interpolation.

2. Generator Network (G)
The generator is a deep ResNet-based network designed to upsample the image and restore fine details.

Residual-in-Residual Dense Blocks (RRDBs):
These are the core building blocks of the generator. Each RRDB combines:

Dense connections (for better feature reuse)

Residual connections (for stable training)

No Batch Normalization (to preserve range flexibility)

Upsampling Layers:
After the RRDBs, pixel-shuffle layers are used to upscale the image (e.g., 2x, 4x).

_Output: A high-resolution (HR) image generated from the LR input._

3. Discriminator Network (D)
The discriminator is trained adversarially to distinguish between real high-resolution images and those generated by the generator.

PatchGAN-style architecture:
Focuses on texture details by analyzing image patches instead of the whole image.

üìç_Goal: Encourage the generator to produce more realistic textures._

4. Perceptual Loss (VGG Feature Loss)
Instead of just relying on pixel-wise losses (e.g., MSE), ESRGAN uses a pre-trained VGG19 network to compute perceptual loss.

Features are extracted from intermediate layers of VGG

Ensures the generated image is perceptually close to the ground truth

Leads to better texture and visual fidelity

5. Training Objective (Adversarial + Content + Perceptual Loss)
The generator is trained using a weighted combination of:

Content Loss (pixel-wise MSE or L1)

Perceptual Loss (VGG feature space difference)

Adversarial Loss (from the discriminator)

Final Output: Super-Resolved Image
The trained generator can upscale LR images to HR with fine details and realism, suitable for use in photography, video enhancement, and restoration tasks.

## model network architecture 
![Image](https://github.com/user-attachments/assets/937983ff-c76a-40b3-b0ea-5272251420a7)

# Hardware Requirements

To run the full pipeline, the following hardware specifications are recommended:

| Model         | Resolution (H √ó W √ó Frames) | GPU Peak Memory |
|---------------|-----------------------------|------------------|
| HunyuanVideo  | 720 √ó 1280 √ó 129            | 60 GB            |
| HunyuanVideo  | 544 √ó 960 √ó 129             | 45 GB            |

Note: A minimum of NVIDIA RTX 6000 GPU with 50GB memory is recommended.

# Sample Prompt and Output
Prompt: "Create a 6-second cinematic video clip featuring a stylish human figure. The person should be wearing modern, luxurious sunglasses (reflective lenses, metallic frames) and exhibit an extravagant, fashionable dress sense: think designer clothes with bold patterns, layered textures, and accessorized with jewelry like rings, bracelets, and a sleek watch. The clothing should blend streetwear and high-fashion styles, like a bright patterned blazer, tailored pants, and exclusive sneakers or leather shoes. The human should have a confident walking posture or a slow-motion head turn, showing off the sunglasses prominently. The scene should have excellent lighting that highlights clothing details and lens reflections, with a high-definition urban or upscale city background. Focus heavily on realistic skin textures, fabric movement, light glinting off accessories, and natural hair flow (if visible). Overall tone: chic, modern, luxurious."

Generated Video:
https://github.com/user-attachments/assets/d1c94a1a-2861-45d2-a977-f432470e6296

prompt: "Create a short video in the style of a fresh, USA-style film photo. The scene features a young American woman standing casually on a quiet urban street during golden hour. She is holding a coffee cup in one hand and wearing a textured green crossbody bag from Victoria's Secret. Her outfit includes a loose grey sweater, and her natural, flowing hair catches the soft light. She has a gentle smile and her eyes look sideways at the camera, conveying a peaceful, candid mood. The background shows softly blurred buildings and street elements, bathed in the warm, natural sunlight of a setting sun. The composition should be a mid-range, half-length portrait with soft, cinematic light and subtle shadows that enhance the calm, serene atmosphere. The camera angle should feel natural and slightly handheld, evoking a film-like, authentic moment."

Generated Video: 
https://github.com/user-attachments/assets/87f54fd1-c217-49c7-94a0-6b82243306da


